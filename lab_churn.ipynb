{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.8", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 2, 
  "cells": [
    {
      "source": [
        "# Comparing and evaluating models\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "hide": true
      }
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")\n", 
        "from PIL import Image"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import GridSearchCV\n", 
        "from sklearn.model_selection import train_test_split\n", 
        "from sklearn.metrics import confusion_matrix\n", 
        "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n", 
        "    if score_func:\n", 
        "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n", 
        "    else:\n", 
        "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n", 
        "    gs.fit(X, y)\n", 
        "    #print(\"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_)\n", 
        "    best = gs.best_estimator_\n", 
        "    return best\n", 
        "def do_classify(clf, parameters, indf, featurenames, targetname, target1val,mode=\"mask\", reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n", 
        "    subdf=indf[featurenames]\n", 
        "    X=subdf.values\n", 
        "    y=(indf[targetname].values==target1val)*1\n", 
        "    if mode==\"mask\":\n", 
        "        print(\"using mask\")\n", 
        "        mask=reuse_split\n", 
        "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n", 
        "    else:\n", 
        "        print(\"using reuse split\")\n", 
        "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n", 
        "    if parameters:\n", 
        "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n", 
        "    clf=clf.fit(Xtrain, ytrain)\n", 
        "    training_accuracy = clf.score(Xtrain, ytrain)\n", 
        "    test_accuracy = clf.score(Xtest, ytest)\n", 
        "    print(\"############# based on standard predict ################\")\n", 
        "    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n", 
        "    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n", 
        "    print(confusion_matrix(ytest, clf.predict(Xtest)))\n", 
        "    print(\"########################################################\")\n", 
        "    return clf, Xtrain, ytrain, Xtest, ytest"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "from matplotlib.colors import ListedColormap\n", 
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n", 
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n", 
        "cm = plt.cm.RdBu\n", 
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", 
        "\n", 
        "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False):\n", 
        "    h = .02\n", 
        "    X=np.concatenate((Xtr, Xte))\n", 
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", 
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", 
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n", 
        "                         np.linspace(y_min, y_max, 100))\n", 
        "\n", 
        "    #plt.figure(figsize=(10,6))\n", 
        "    if mesh:\n", 
        "        if zfunc:\n", 
        "            p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n", 
        "            p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "            Z=zfunc(p0, p1)\n", 
        "        else:\n", 
        "            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", 
        "        Z = Z.reshape(xx.shape)\n", 
        "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light, alpha=alpha, axes=ax)\n", 
        "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n", 
        "    # and testing points\n", 
        "    yact=clf.predict(Xte)\n", 
        "    ax.scatter(Xte[:, 0], Xte[:, 1], c=yte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n", 
        "    ax.set_xlim(xx.min(), xx.max())\n", 
        "    ax.set_ylim(yy.min(), yy.max())\n", 
        "    return ax,xx,yy"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true, 
        "hide": true
      }
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n", 
        "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha) \n", 
        "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", 
        "    Z = Z.reshape(xx.shape)\n", 
        "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n", 
        "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n", 
        "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14,)\n", 
        "    return ax "
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "source": [
        "## The churn example\n", 
        "\n", 
        "This is a dataset from a telecom company, of their customers. Based on various features of these customers and their calling plans, we want to predict if a customer is likely to leave the company. This is expensive for the company, as a lost customer means lost monthly revenue!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "#data set from yhathq: http://blog.yhathq.com/posts/predicting-customer-churn-with-sklearn.html\n", 
        "dfchurn=pd.read_csv(\"data/churn.csv\")\n", 
        "dfchurn.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets write some code to feature select and clean our data first, of-course."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "dfchurn[\"Int'l Plan\"] = dfchurn[\"Int'l Plan\"]=='yes'\n", 
        "dfchurn[\"VMail Plan\"] = dfchurn[\"VMail Plan\"]=='yes'"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 7, 
      "cell_type": "code", 
      "source": [
        "colswewant_cont=[ u'Account Length', u'VMail Message', u'Day Mins', u'Day Calls', u'Day Charge', u'Eve Mins', u'Eve Calls', u'Eve Charge', u'Night Mins', u'Night Calls', u'Night Charge', u'Intl Mins', u'Intl Calls', u'Intl Charge', u'CustServ Calls']\n", 
        "colswewant_cat=[u\"Int'l Plan\", u'VMail Plan']"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "## Asymmetry"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "First notice that our data set is very highly asymmetric, with positives, or people who churned, only making up 14-15% of the samples.\n", 
        "\n", 
        "YOUR TURN NOW\n", 
        "\n", 
        "> Write code to show this"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This means that a classifier which predicts that EVERY customer is a negative (does not churn) has an accuracy rate of 85-86%. \n", 
        "\n", 
        "But is accuracy the correct metric?\n", 
        "\n", 
        "## Train-test Split"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "churntrain, churntest = train_test_split(range(dfchurn.shape[0]), train_size=0.6)\n", 
        "churnmask=np.ones(dfchurn.shape[0], dtype='int')\n", 
        "churnmask[churntrain]=1\n", 
        "churnmask[churntest]=0\n", 
        "churnmask = (churnmask==1)\n", 
        "churnmask"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "testsize = dfchurn[~churnmask].shape[0]\n", 
        "testchurners=dfchurn['Churn?'][~churnmask].values=='True.'"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## Remember the Confusion matrix? We reproduce it here for convenience"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n", 
        "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n", 
        "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n", 
        "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n", 
        "\n", 
        "A classifier produces a confusion matrix which looks like this:\n", 
        "\n", 
        "![hwimages](./images/confusionmatrix.png)\n", 
        "\n", 
        "\n", 
        "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the form above, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`\n", 
        "\n", 
        "Consider two classifiers, A and B, as in the image below. Suppose they were trained on a balanced set. Let A make its mistakes only through false positives: non-churners(n) predicted to churn(Y), while B makes its mistake only through false negatives, churners(p), predicted not to churn(N). Now consider what this looks like on an unbalanced set, where the ps (churners) are much less than the ns (non-churners). It would seem that B makes far fewer misclassifications based on accuracy than A, and would thus be a better classifier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "![m:abmodeldiag](./images/abmodeldiag.png)\n", 
        "\n", 
        "However, is B reaslly the best classifier for us? False negatives are people who churn, but we predicted them not to churn.These are very costly for us. So for us. classifier A might be better, even though, on the unbalanced set, it is way less accurate!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Classifiers should be about the Business End: keeping costs down"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Establishing Baseline Classifiers via profit or loss."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Whenever you are comparing classifiers you should always establish a baseline, one way or the other.  In our churn dataset there are two obvious baselines: assume every customer wont churn, and assume all customers will churn.\n", 
        "\n", 
        "The former baseline, will on our dataset, straight away give you a 85.5% accuracy. If you are planning on using accuracy, any classifier you write ought to beat this. The other baseline, from an accuracy perspective is less interesting: it would only have a 14.5% correct rate.\n", 
        "\n", 
        "But as we have seen, on such asymmetric data sets, accuracy is just not a good metric. So what should we use?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "**A metric ought to hew to the business function that the classifier is intended for**.\n", 
        "\n", 
        "In our case, we want to minimize the cost/maximize the profit for the telecom.\n", 
        "\n", 
        "But to do this we need to understand the business situation. To do this, we write a **utility**, or, equivalently, **cost** matrix associated with the 4 scenarios that the confusion matrix talks about. \n", 
        "\n", 
        "![cost matrix](images/costmatrix.png)\n", 
        "\n", 
        "Remember that +ives or 1s are churners, and -ives or 0s are the ones that dont churn. \n", 
        "\n", 
        "Lets assume we make an offer with an administrative cost of 3 and an offer cost of 100, an incentive for the customer to stay with us. If a customer leaves us, we lose the customer lifetime value, which is some kind of measure of the lost profit from that customer. Lets assume this is the average number of months a customer stays with the telecom times the net revenue from the customer per month. We'll assume 3 years and 30/month margin per user lost, for roughly a 1000 loss."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "admin_cost=3\n", 
        "offer_cost=100\n", 
        "clv=1000#customer lifetime value"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "- TN=people we predicted not to churn who wont churn. We associate no cost with this as they continue being our customers\n", 
        "- FP=people we predict to churn. Who wont. Lets associate a `admin_cost+offer_cost` cost per customer with this as we will spend some money on getting them not to churn, but we will lose this money.\n", 
        "- FN=people we predict wont churn. And we send them nothing. But they will. This is the big loss, the `clv`\n", 
        "- TP= people who we predict will churn. And they will. These are the people we can do something with. So we make them an offer. Say a fraction f accept it. Our cost is\n", 
        "\n", 
        "`f * (offer_cost+admin_cost) + (1-f)*(clv+admin_cost)`\n", 
        "\n", 
        "This model can definitely be made more complex.\n", 
        "\n", 
        "Lets assume a conversion fraction of 0.5"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 22, 
      "cell_type": "code", 
      "source": [
        "conv=0.5\n", 
        "tnc = 0.\n", 
        "fpc = admin_cost+offer_cost\n", 
        "fnc = clv\n", 
        "tpc = conv*offer_cost + (1. - conv)*(clv) + admin_cost"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 23, 
      "cell_type": "code", 
      "source": [
        "cost=np.array([[tnc,fpc],[fnc, tpc]])\n", 
        "cost"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We can compute the average cost(profit) per person using the following formula, which calculates the \"expected value\" of the per-customer loss/cost(profit):\n", 
        "\n", 
        "\\begin{eqnarray}\n", 
        "Cost &=& c(1P,1A) \\times p(1P,1A) + c(1P,0A) \\times p(1P,0A) + c(0P,1A) \\times p(0P,1A) + c(0P,0A) \\times p(0P,0A) \\\\\n", 
        "&=& \\frac{TP \\times c(1P,1A) + FP \\times c(1P,0A) + FN \\times c(0P,1A) + TN \\times c(0P,0A)}{N}\n", 
        "\\end{eqnarray}\n", 
        "\n", 
        "where N is the total size of the test set, 1P is predictions for class 1, or positives, 0A is actual values of the negative class in the test set. The first formula above just weighs the cost of a combination of observed and predicted with the out-of-sample probability of the combination occurring. The probabilities are \"estimated\" by the corresponding confusion matrix on the test set. (We'll provide a proof of this later in the course for the mathematically inclined, or just come bug Rahul at office hour if you cant wait!)\n", 
        "\n", 
        "The cost can thus be found by multiplying the cost matrix by the confusion matrix elementwise, and dividing by the sum of the elements in the confusion matrix, or the test set size.\n", 
        "\n", 
        "We implement this process of finding the average cost per person in the `average_cost` function below:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "def average_cost(y, ypred, cost):\n", 
        "    c=confusion_matrix(y,ypred)\n", 
        "    score=np.sum(c*cost)/np.sum(c)\n", 
        "    return score"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "#### No customer churns and we send nothing\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "ypred_dste = np.zeros(testsize, dtype=\"int\")\n", 
        "confusion_matrix(testchurners, ypred_dste)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "dsteval=average_cost(testchurners, ypred_dste, cost)\n", 
        "dsteval"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### All customers churn, we send everyone\n", 
        "\n", 
        "YOUR TURN NOW\n", 
        "\n", 
        "> get the confusion matrix on test set"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "> calculate the average cost in the variable `steval`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "steval=average_cost(testchurners, ypred_ste, cost)\n", 
        "steval"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Make offers to everyone costs us even more, not surprisingly. The first one is the one to beat!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Naive Bayes Classifier\n", 
        "\n", 
        "So lets try a classifier. Here we try one known as Gaussian Naive Bayes. We'll just use the default parameters, since the actual details are not of importance to us.\n", 
        "\n", 
        "YOUR CODE HERE\n", 
        "\n", 
        "> Use `GaussianNB` from `sklearn.naive_bayes`. Use `do_classify`to output `clfgnb` the classifier, and `Xtrain, ytrain, Xtest, ytest`. You will need to set `mode` to mask and use churnmask in the `reuse_split`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "confusion_matrix(ytest, clfgnb.predict(Xtest))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "average_cost(ytest, clfgnb.predict(Xtest), cost)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Ok! We did better! Seems much better! But is this the true value of our cost? To answer this question, we need to ask a question: what exactly is `clf.predict` doing?\n", 
        "\n", 
        "How is it using probabilities? And what if I have classifiers that dont return probabilities?\n", 
        "\n", 
        "For both these reasons we turn to ROC curves."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Changing the Prediction threshold, and the ROC Curve"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Our dataset is a very lopsided data set with 86% of samples being negative. We now know that in such a case, accuracy is not a very good measure of a classifier.\n", 
        "\n", 
        "We have also noticed that, as is often the case in situations in which one class dominates the other, the costs of one kind of misclassification: false negatives are differently expensive than false positives. We saw above that FN are more costly in our case than FP. \n", 
        "\n", 
        "\n", 
        "In the case of such asymmetric costs, the `sklearn` API function `predict` is useless, as it assumes a threshold probability of having a +ive sample to be 0.5; that is, if a sample has a greater than 0.5 chance of being a 1, assume it is so. Clearly, when FN are more expensive than FP, you want to lower this threshold: you are ok with falsely classifying -ive examples as +ive. We play with this below by chosing a threshold `t` in the function `repredict` which chooses a different threshold than 0.5 to make a classification.\n", 
        "\n", 
        "You can think about this very starkly from the perspective of the cancer doctor. Do you really want to be setting a threshold of 0.5 probability to predict if a patient has cancer or not? The false negative problem: ie the chance you predict someone dosent have cancer who has cancer is much higher for such a threshold. You could kill someone by telling them not to get a biopsy. Why not play it safe and assume a much lower threshold: for eg, if the probability of 1(cancer) is greater than 0.05, we'll call it a 1.\n", 
        "\n", 
        "One caveat: we cannot repredict for the linear SVM model `clfsvm`, as the SVM is whats called a \"discriminative\" classifier: it directly gives us a decision function, with no probabilistic explanation and no probabilities. (I lie, an SVM can be retrofitted with probabilities: see http://scikit-learn.org/stable/modules/svm.html#scores-probabilities, but these are expensive amd not always well callibrated).\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "def repredict(est,t, xtest):\n", 
        "    probs=est.predict_proba(xtest)\n", 
        "    p0 = probs[:,0]\n", 
        "    p1 = probs[:,1]\n", 
        "    ypred = (p1 >= t)*1\n", 
        "    return ypred"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "average_cost(ytest, repredict(clfgnb, 0.3, Xtest), cost)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 35, 
      "cell_type": "code", 
      "source": [
        "ts = np.linspace(0,1,100)\n", 
        "plt.plot(ts, [average_cost(ytest, repredict(clfgnb, t, Xtest), cost) for t in ts])"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "execution_count": 36, 
      "cell_type": "code", 
      "source": [
        "plt.hist(clfgnb.predict_proba(Xtest)[:,1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Aha! At a 0.3 threshold we save more money!\n", 
        "\n", 
        "We see that in this situation, where we have asymmetric costs, we do need to change the threshold at which we make our positive and negative predictions. We need to change the threshold so that we much dislike false negatives (same in the cancer case). Thus we must accept many more false positives by setting such a low threshold.\n", 
        "\n", 
        "For otherwise, we let too many people slip through our hands who would have stayed with our telecom company given an incentive. But how do we pick this threshold?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The ROC Curve"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "ROC curves are actually a set of classifiers, in which we move the threshold for classifying a sample as positive from 0 to 1. (In the standard scenario, where we use classifier accuracy, this threshold is implicitly set at 0.5).\n", 
        "\n", 
        "We talked more about how to create a ROC curve in the accompanying lab to this one, so here we shall just repeat the ROC curve making code from there."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 37, 
      "cell_type": "code", 
      "source": [
        "from sklearn.metrics import roc_curve, auc"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 38, 
      "cell_type": "code", 
      "source": [
        "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n", 
        "    initial=False\n", 
        "    if not ax:\n", 
        "        ax=plt.gca()\n", 
        "        initial=True\n", 
        "    if proba:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n", 
        "    else:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n", 
        "    roc_auc = auc(fpr, tpr)\n", 
        "    if skip:\n", 
        "        l=fpr.shape[0]\n", 
        "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    else:\n", 
        "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n", 
        "    label_kwargs = {}\n", 
        "    label_kwargs['bbox'] = dict(\n", 
        "        boxstyle='round,pad=0.3', alpha=0.2,\n", 
        "    )\n", 
        "    for k in range(0, fpr.shape[0],labe):\n", 
        "        #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n", 
        "        threshold = str(np.round(thresholds[k], 2))\n", 
        "        ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n", 
        "    if initial:\n", 
        "        ax.plot([0, 1], [0, 1], 'k--')\n", 
        "        ax.set_xlim([0.0, 1.0])\n", 
        "        ax.set_ylim([0.0, 1.05])\n", 
        "        ax.set_xlabel('False Positive Rate')\n", 
        "        ax.set_ylabel('True Positive Rate')\n", 
        "        ax.set_title('ROC')\n", 
        "    ax.legend(loc=\"lower right\")\n", 
        "    return ax"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 46, 
      "cell_type": "code", 
      "source": [
        "plt.figure(figsize=(12,9))\n", 
        "make_roc(\"gnb\",clfgnb, ytest, Xtest, None, labe=50)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "OK. Now that we have a ROC curve that shows us different thresholds, we need to figure how to pick the appropriate threshold from the ROC curve. But first, let us try another classifier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Classifier Comparison"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Decision Trees"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Descision trees are very simple things we are all familiar with. If a problem is multi-dimensional, the tree goes dimension by dimension and makes cuts in the space to create a classifier.\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 40, 
      "cell_type": "code", 
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 41, 
      "cell_type": "code", 
      "source": [
        "reuse_split=dict(Xtrain=Xtrain, Xtest=Xtest, ytrain=ytrain, ytest=ytest)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We train a simple decision tree classifier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "clfdt=DecisionTreeClassifier()\n", 
        "clfdt, Xtrain, ytrain, Xtest, ytest  = do_classify(clfdt, {\"max_depth\": list(range(1,10,1))}, dfchurn, colswewant_cont+colswewant_cat, 'Churn?', \"True.\", \n", 
        "                                                   mode=\"not mask\", reuse_split=reuse_split)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "confusion_matrix(ytest,clfdt.predict(Xtest))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Compare!\n", 
        "\n", 
        "YOUR TURN HERE\n", 
        "\n", 
        "> Use make_roc to compare the ROC curbes. Use `labe=1` for the decision tree ROC"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 45, 
      "cell_type": "code", 
      "source": [
        "plt.figure(figsize=(12,9))\n", 
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "How do we read which classifier is better from a ROC curve. The usual advice is to go to the North-West corner of a ROC curve, as that is closest to TPE=1, FPR=0. But thats not our setup here..we have this asymmetric data set. The other advice is to look at the classifier with the highest AUC. But as we can see in the image the AUC is nearly the same, but the classifiers seem to have very different performances in different parts of the graph\n", 
        "\n", 
        "\n", 
        "And then there is the question of figuring what threshold to choose as well. To answer both of these, we are going to have to turn back to cost"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Reprediction again: Now with Cost or Risk"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "You can use the utility or risk matrix to provide a  threshold to pick for our classifier. \n", 
        "\n", 
        "The key idea is that we want to minimize cost on our test set, so for each sample, simply pick the class which does that. \n", 
        "\n", 
        "Decision Theory is the branch of statistics that speaks to this: its the theory which tells us how to make a positive or negative prediction for a given sample.\n", 
        "\n", 
        "There is a second cost or risk or loss involved in machine learning. This is the decision loss.\n", 
        "\n", 
        "What do we mean by a \"decision\" exactly? We'll use the letter g here to indicate a decision, in both the regression and classification problems. In the classification problem, one example of a decision is the process used to choose the class of a sample, given the probability of being in that class. As another example, consider the cancer story from the previous chapter. The decision may be: ought we biopsy, or ought we not biopsy. By minimizing the estimation risk, we obtain a probability that the patient has cancer. We must mix these probabilities with \"business knowledge\" or \"domain knowledge\" to make a decision.\n", 
        "\n", 
        "(As an aside, this is true in regression as well. there are really two losses there. The first one, the one equivalent to the log loss is the one where we say that at each point the prediction for y is a gaussian....the samples of this gaussian come from the bootstrap we make on the original data set...each replication leads to a new line and a distribution for the prediction at a point x. But usually in a regression we just quote the mean of this distribution at each point, the regression line E[y|x]. Why the mean? The mean comes from choosing a least squares decision loss...if we chose a L1 loss, we'd be looking at a median.)\n", 
        "\n", 
        "**The cost matrix we have been using above is exactly what goes into this decision loss!!**\n", 
        "\n", 
        "### Decision Theory Math\n", 
        "\n", 
        "To understand this, lets follow through with a bit of math:\n", 
        "(you can safely skip this section if you are not interested)\n", 
        "\n", 
        "We simply weigh each combinations loss by the probability that that combination can happen:\n", 
        "\n", 
        "$$ R_{g}(x) = \\sum_y l(y,g(x)) p(y|x)$$\n", 
        "\n", 
        "That is, we calculate the **average risk** over all choices y, of making choice g for a given sample.\n", 
        "\n", 
        "Then, if we want to calculate the overall risk, given all the samples in our set, we calculate:\n", 
        "\n", 
        "$$R(g) = \\sum_x p(x) R_{g}(x)$$\n", 
        "\n", 
        "It is sufficient to minimize the risk at each point or sample to minimize the overall risk since $p(x)$ is always positive.\n", 
        "\n", 
        "Consider the two class classification case. Say we make a \"decision g about which class\" at a sample x. Then:\n", 
        "\n", 
        "$$R_g(x) = l(1, g)p(1|x) + l(0, g)p(0|x).$$\n", 
        "\n", 
        "Then for the \"decision\" $g=1$ we have:\n", 
        "\n", 
        "$$R_1(x) = l(1,1)p(1|x) + l(0,1)p(0|x),$$\n", 
        "\n", 
        "and for the \"decision\" $g=0$ we have:\n", 
        "\n", 
        "$$R_0(x) = l(1,0)p(1|x) + l(0,0)p(0|x).$$\n", 
        "\n", 
        "Now, we'd choose $1$ for the sample at $x$ if:\n", 
        "\n", 
        "$$R_1(x) \\lt R_0(x).$$\n", 
        "\n", 
        "$$ P(1|x)(l(1,1) - l(1,0)) \\lt p(0|x)(l(0,0) - l(0,1))$$\n", 
        "\n", 
        "This gives us a ratio `r` between the probabilities to make a prediction. We assume this is true for all samples.\n", 
        "\n", 
        "So, to choose '1':\n", 
        "\n", 
        "$$p(1|x) \\gt r P(0|x) \\implies r=\\frac{l(0,1) - l(0,0)}{l(1,0) - l(1,1)} =\\frac{c_{FP} - c_{TN}}{c_{FN} - c_{TP}}$$\n", 
        "\n", 
        "This may also be written as:\n", 
        "\n", 
        "$$P(1|x) \\gt t = \\frac{r}{1+r}$$.\n", 
        "\n", 
        "If you assume that True positives and True negatives have no cost, and the cost of a false positive is equal to that of a false positive, then $r=1$ and the threshold is the usual intutive $t=0.5$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 39, 
      "cell_type": "code", 
      "source": [
        "cost"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 40, 
      "cell_type": "code", 
      "source": [
        "def rat(cost):\n", 
        "    return (cost[0,1] - cost[0,0])/(cost[1,0]-cost[1,1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "rat(cost)"
      ], 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "execution_count": 41, 
      "cell_type": "code", 
      "source": [
        "def c_repredict(est, c, xtest):\n", 
        "    r = rat(c)\n", 
        "    t=r/(1.+r)\n", 
        "    print(\"r=\", r, \"t=\", t)\n", 
        "    probs=est.predict_proba(xtest)\n", 
        "    p0 = probs[:,0]\n", 
        "    p1 = probs[:,1]\n", 
        "    ypred = (p1 >= t)*1\n", 
        "    return ypred"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 45, 
      "cell_type": "code", 
      "source": [
        "average_cost(ytest, c_repredict(clfdt, cost, Xtest), cost)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This value turns out to be only approximate, and we are better using a ROC curve or a Cost curve (below) to find minimum cost. However, it will get us in the right ballpark of the threshold we need. Note that the threshold itself depends only on costs and is independent of the classifier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Cost curves"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The proof is always in the pudding. Why not just plot the cost/profit (per person) per threshold on a ROC like curve to see which classifier maximizes profit/minimizes cost? \n", 
        "\n", 
        "Just like in a ROC curve, we go down the sorted (by score or probability) list of samples. We one-by-one add an additional sample to our positive samples, noting down the attendant classifier's TPR and FPR and threshold. In addition to what we do for the ROC curve, we now also note down the percentage of our list of samples predicted as positive. Remember we start from the mostest positive, where the percentage labelled as positive would be minuscule, like 0.1 or so and the threshold like a 0.99 in probability or so. As we decrease the threshold, the percentage predicted to be positive clearly increases until everything is predicted positive at a threshold of 0. What we now do is, at each such additional sample/threshold (given to us by the `roc_curve` function from `sklearn`), we calculate the expected profit per person and plot it against the percentage predicted positive by that threshold to produce a profit curve. Thus, small percentages correspond to samples most likely to be positive: a percentage of 8% means the top 8% of our samples ranked by likelihood of being positive.\n", 
        "\n", 
        "As in the ROC curve case, we use `sklearn`'s `roc_curve` function to return us a set of thresholds with TPRs and FPRs."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 47, 
      "cell_type": "code", 
      "source": [
        "def percentage(tpr, fpr, priorp, priorn):\n", 
        "    perc = tpr*priorp + fpr*priorn\n", 
        "    return perc\n", 
        "def av_cost2(tpr, fpr, cost, priorp, priorn):\n", 
        "    profit = priorp*(cost[1][1]*tpr+cost[1][0]*(1.-tpr))+priorn*(cost[0][0]*(1.-fpr) +cost[0][1]*fpr)\n", 
        "    return profit\n", 
        "def plot_cost(name, clf, ytest, xtest, cost, ax=None, threshold=False, labe=200, proba=True):\n", 
        "    initial=False\n", 
        "    if not ax:\n", 
        "        ax=plt.gca()\n", 
        "        initial=True\n", 
        "    if proba:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n", 
        "    else:\n", 
        "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n", 
        "    priorp=np.mean(ytest)\n", 
        "    priorn=1. - priorp\n", 
        "    ben=[]\n", 
        "    percs=[]\n", 
        "    for i,t in enumerate(thresholds):\n", 
        "        perc=percentage(tpr[i], fpr[i], priorp, priorn)\n", 
        "        ev = av_cost2(tpr[i], fpr[i], cost, priorp, priorn)\n", 
        "        ben.append(ev)\n", 
        "        percs.append(perc*100)\n", 
        "    ax.plot(percs, ben, '-', alpha=0.3, markersize=5, label='cost curve for %s' % name)\n", 
        "    if threshold:\n", 
        "        label_kwargs = {}\n", 
        "        label_kwargs['bbox'] = dict(\n", 
        "        boxstyle='round,pad=0.3', alpha=0.2,\n", 
        "        )\n", 
        "        for k in range(0, fpr.shape[0],labe):\n", 
        "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n", 
        "            threshold = str(np.round(thresholds[k], 2))\n", 
        "            ax.annotate(threshold, (percs[k], ben[k]), **label_kwargs)\n", 
        "    ax.legend(loc=\"lower right\")\n", 
        "    return ax"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 50, 
      "cell_type": "code", 
      "source": [
        "plt.figure(figsize=(12,9))\n", 
        "ax = plot_cost(\"gnb\",clfgnb, ytest, Xtest, cost, threshold=True, labe=50);\n", 
        "plot_cost(\"dt\",clfdt, ytest, Xtest, cost, ax, threshold=True, labe=2);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note the customers on the left of this graph are most likely to churn (be positive).\n", 
        "\n", 
        "This if you had a finite budget, you should be targeting them!\n", 
        "\n", 
        "Finding the best classifier has a real consequence: you save money!!!\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 51, 
      "cell_type": "code", 
      "source": [
        "cost"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The above graph is a snapshot of a run. One thing worth noticing is that classifiers perform differently in different regions. If you targeted only the top 20% of your users..and these are the ones most likely to churn so you should target them first, you would want to use the decision-tree classifier. And you might only get to target these top 20 given your budget. Remember that there is a cost associated with targeting predicted positives. That cost can be read of the graph above. Say we had a million customers. Now, at 10%, or 100,000 we are talking about a minimum budget of 10.3 million dollars. \n", 
        "\n", 
        "If 10-15 million is your budget, then you use the decision tree classifier on your left. If 40-60 million is your budget, roughly, you would use the gnb classifier instead."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ]
}